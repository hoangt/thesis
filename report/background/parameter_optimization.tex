\section{Global Optimization}

Optimization is a field of applied mathematics that deals with finding the best
set of parameters to optimize an objective function. A problem with $N$
variables $\{n_0, \dots, n_{N-1}\}$ in range $n_i \in \{0, \dots, k-1\}$ will
have a search space with $k^N$ possible solutions. Each feasible solution must
be evaluated to find how well it performs, this is called a solutions
\emph{fitness}.

In general, the possible solution can be thought of as a space that has two or
more dimensions, directly related to the number of variables in the solution
in addition to one axis for calculated fitness. If a problem contains a single
variable, it's solution space might look like \autoref{fig:fitnesslandscape}
where the X-axis is the value of the single variable, while the Y-axis is the
fitness of this solution. Problems with more variables will have more axis. The
optimization algorithms can be thought of as methods for exploring and finding
the highest or lowest point in of the fitness-axis.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/Fitness-landscape-cartoon.png}
    \caption{Sketch of a fitness landscape, borrowed from \cite{wikifitnesslandscape}}
    \label{fig:fitnesslandscape}
\end{figure}

As both $k$ and $N$ grow large, it becomes infeasible to search
through and evaluate the vast amount of permutations. In such cases, other
techniques must be employed. Finding an arbitrary local minimum is often
straight forward using classical \emph{local} optimization methods such as the
Simple Hill Climbing Algorithm. However, these methods cannot always be used to
find a global minimum\cite{russellnorvig}. A wide range of algorithms to search
through a subset of the solution space exists, each with different approaches
and properties. \autoref{fig:metaheuristics} shows a map of how different
optimization algorithms are related to each other. Most of these are described
in detail in \cite{russellnorvig}.

On a high level, optimization algorithms can be divided into deterministic and
non-deterministic approaches. The deterministic approaches can be thought of as
a single path from a starting point to the best solution, while the
non-deterministic approaches usually selects one or more paths which may or may
not be different on each run. Deterministic algorithms will always find the same
solution, and if the search space is large enough, it might take very long time
to find a decent answer. When using non-deterministic algorithms, they tend to
have a more explorative behaviour; each computation of the next state includes
some form of randomness. E.g. simulated annealing will do the same as the hill
climbing algorithm, but will have some chance of moving downhill, thus it will
be less prone to be stuck in a local minima.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/630px-Metaheuristics_classification.png}
    \caption{Different classifications of metaheuristics, borrowed from Johann "nojhan" Dr√©o and Caner Candan\cite{wikimetaheuristics}}
    \label{fig:metaheuristics}
\end{figure}

\begin{description}
    \item[Simulated Annealing] is a technique that belongs to the field of
        stochastic optimization and metaheuristics, inspired by the process of
        annealing in metallurgy. Initially, it starts in a random state $s$ and
        for each iteration it probabilistically decides between moving the
        system to a neighboring state $s'$ or staying in in state $s$. To avoid
        ending up in a local minima, the probability starts high, but decreases
        after time. Hence, simulated annealing quickly considers the most
        important parts of the state.
    \item[Evolutionary Algorithms] is a term that refers to computational
        methods inspired by the process and mechanisms of biological evolution.
        Many variations exists, some of which can be used to solve optimization
        problems.

\end{description}

