\section{Results}

We have now discussed how PET must be tuned to work with satisfying accuracy.
Here we present the results obtained by tuning towards the ARM Cortex-A9.

\subsection{gem5 CPU Model Accuracy}

We modeled different CPU configurations for gem5 and achieved runtime in
milliseconds as tabulated in \autoref{tbl:gem5runtimeaccuracy}. Each test was
run with the command in \autoref{lst:gem5commandline}, with \texttt{\$CPU}
changed to \texttt{exynos\_4412p}, \texttt{arm\_detailed} and \texttt{timing}.

\begin{lstlisting}[float=htb,language=sh,numbers=none,label={lst:gem5commandline},caption={gem5
Command Line.}]
$ build/ARM/gem5.opt --remote-gdb-port=0 -d m5out
    configs/example/se.py -c bin --cpu-type=$CPU
    --mem-type=LPDDR2_S4_800_x32 --sys-clock=440MHz
    --cpu-clock=1700MHz --num-l3caches=0 --caches --l2cache
    --l2_assoc=16 --l2_size=1MB --l1d_size=32kB
    --mem-size=2048MB --l1d_assoc=4 --l1i_assoc=4
\end{lstlisting}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                & Pi      & SHA-512 & Trend   & SubMul \\
\hline
Real hardware   & 13.500  & 22.600  & 14.600  & 28.200 \\
exynos\_4412p   & 13.790  & 22.819  & 11.898  & 23.978 \\
arm\_detailed   &  6.708  & 10.368  &  4.344  &  8.653 \\
timing          & 19.564  & 40.659  & 20.503  & 41.964 \\
\hline
\end{tabular}
\caption{gem5 runtime accuracy (O3 with classic memory system). In milliseconds.}
\label{tbl:gem5runtimeaccuracy}
\end{table}

As can be seen, we were able to get the runtime of our modified O3
model (\texttt{exynos\_4412p}) fairly close. With ordinary workloads Pi and
SHA-512, the execution time differed by less
than 2.2~\%. Trend and SubMul, the synthetic tests, differed by 22.7~\% and
17.6~\%, respectively.


\subsection{Optimization using $1 + \lambda$}

PET has been optimized by a genetic inspired algorithm, so it was expected to
perform very well on the training data sets. \autoref{lst:weights} lists out the
final weights found by the GA. The weights have reasonable values for the ALU
operations (\texttt{IntAlu, IntMult, MemRead, MemWrite, SimdFloatMisc}) compared
to the results found in \cite{rundehvatum2013exploring}. The values for the
cache hierarchy (\texttt{L1DR, L1DW, L1IR, L1IW, L2R, L2W}) does also seem
adequate. \texttt{PhysR} and \texttt{PhysW} are set to zero because the physical
memory of the Exynos~4412~Prime is not believed to be powered by $V_{core}$.
However, peculiar values are set for \texttt{Idle} and \texttt{Static}. Our
theory is that CPU idle time is often caused by high I/O activity; this would
explain how an idle CPU could use much power. However, we can not be certain how
the events match to the hardware. The GA only optimizes the event weights and
ignores the intended meaning of them. One must also remember that the
\texttt{Idle} events are calculated by a very simple model, and the cost can
be high if too few idle cycles are recognized by PET.

\lstinputlisting[caption={Final weight configuration.},label={lst:weights},float,language=Python]{../pet/weights.conf}

Results for each training set after the training session are displayed in
\autoref{fig:pi-training}, \autoref{fig:sha2-training},
\autoref{fig:trend-training} and \autoref{fig:submul-training}. Each figure
represents one of the workloads explained in \autoref{sec:workloads}. The red
line in each figure represents the prediction by PET, while the green line is a
direct plot of readings from the test setup. To remedy the mismatch in execution
time, the prediction done by PET is stretched.


\subsubsection{Training Set: \texttt{Pi}}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/training/pi-pi.pdf}
    \caption{Overlay of PET training results (red) and training data (green) for the Pi test.}
    \label{fig:pi-training}
\end{figure}

The \texttt{Pi} test uses floating point and random values with a static seed. The
program is compiled with soft-float, so it can be viewed as an integer stress
test. We obtain rather good accuracy with this test, in general the error is a
below $10~mA$. As it represents a common workload, this is a very promising
result. It is interesting that the beginning and the end of the program are the
most power consuming pieces. This is probably due to the simplistic method idle
time is calculated, and the out-of-order nature of the processor makes the
genetic algorithm overestimate the cost of idle.

\newpage

\subsubsection{Training Set: \texttt{Trend}}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/training/trend-trend.pdf}
    \caption{Overlay of PET training results (red) and training data (green) for the Trend test.}
    \label{fig:trend-training}
\end{figure}

The \texttt{Trend} test checks if PET can follow the flow from an ALU intensive
program to a memory intensive program. The accuracy is close to the Pi training
set, but the most important factor with this test is that the drop at $7~ms$ is
resembled in the prediction. Given that PET recognizes this drop, we know that
PET adjusts correctly for the change in instruction flow. The fact that the
sudden drop in power consumption seems to come a bit too slow is most likely due
to discrepancies between the gem5 model and the real hardware.

\newpage

\subsubsection{Training Set: \texttt{SubMul}}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/training/trend-submul.pdf}
    \caption{Overlay of PET training results (red) and training data (green) for the SubMul test.}
    \label{fig:submul-training}
\end{figure}

Further, the \texttt{SubMul} test is created to follow the shift from simple
integer operations to a multiplication-dominated loop. PET again follows the
trend, but overestimates the cost of multiply. Again, the trend is the most
important factor. It should also be noted that even tough the loop contains a
lot of multiply instructions, the assembled binary still uses a lot of basic
integer operations for program flow, storage, registers, addresses, and so on.
Note that the shift now happens earlier in PET than in the hardware, the
opposite of \texttt{Trend}. This is likely due to runtime variatons in gem5
versus real hardware.

\newpage

\subsubsection{Training Set: \texttt{SHA-512}}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{figs/training/sha2-sha2.pdf}
    \caption{Overlay of PET training results (red) and training data (green) for
    the SHA-512 test.}
    \label{fig:sha2-training}
\end{figure}

The last test in the performance tuning algorithm calculates a SHA-512 hash sum.
The test results are shown in \autoref{fig:sha2-training} and differs from the
other tests by being more memory intensive at the same time as it is an
extensive user of both multiply and simpler integer operations. This test is
well within 3~\% of measured and is yet another example of accurate prediction
on general workloads.
